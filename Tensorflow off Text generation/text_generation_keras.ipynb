{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163781\n",
      "Total Vocab:  59\n",
      "Total Patterns:  163681\n"
     ]
    }
   ],
   "source": [
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163681, 100, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2558/2558 [==============================] - ETA: 0s - loss: 2.8241\n",
      "Epoch 00001: loss improved from inf to 2.82406, saving model to weights-improvement-bigger.hdf5\n",
      "2558/2558 [==============================] - 88s 34ms/step - loss: 2.8241\n",
      "Epoch 2/10\n",
      "2557/2558 [============================>.] - ETA: 0s - loss: 2.5454\n",
      "Epoch 00002: loss improved from 2.82406 to 2.54540, saving model to weights-improvement-bigger.hdf5\n",
      "2558/2558 [==============================] - 87s 34ms/step - loss: 2.5454\n",
      "Epoch 3/10\n",
      "2557/2558 [============================>.] - ETA: 0s - loss: 2.3655\n",
      "Epoch 00003: loss improved from 2.54540 to 2.36541, saving model to weights-improvement-bigger.hdf5\n",
      "2558/2558 [==============================] - 87s 34ms/step - loss: 2.3654\n",
      "Epoch 4/10\n",
      "2557/2558 [============================>.] - ETA: 0s - loss: 2.2194\n",
      "Epoch 00004: loss improved from 2.36541 to 2.21931, saving model to weights-improvement-bigger.hdf5\n",
      "2558/2558 [==============================] - 87s 34ms/step - loss: 2.2193\n",
      "Epoch 5/10\n",
      "2557/2558 [============================>.] - ETA: 0s - loss: 2.1167\n",
      "Epoch 00005: loss improved from 2.21931 to 2.11674, saving model to weights-improvement-bigger.hdf5\n",
      "2558/2558 [==============================] - 87s 34ms/step - loss: 2.1167\n",
      "Epoch 6/10\n",
      "2557/2558 [============================>.] - ETA: 0s - loss: 2.0327\n",
      "Epoch 00006: loss improved from 2.11674 to 2.03269, saving model to weights-improvement-bigger.hdf5\n",
      "2558/2558 [==============================] - 89s 35ms/step - loss: 2.0327\n",
      "Epoch 7/10\n",
      "2558/2558 [==============================] - ETA: 0s - loss: 1.9654\n",
      "Epoch 00007: loss improved from 2.03269 to 1.96539, saving model to weights-improvement-bigger.hdf5\n",
      "2558/2558 [==============================] - 89s 35ms/step - loss: 1.9654\n",
      "Epoch 8/10\n",
      "2557/2558 [============================>.] - ETA: 0s - loss: 1.9108\n",
      "Epoch 00008: loss improved from 1.96539 to 1.91081, saving model to weights-improvement-bigger.hdf5\n",
      "2558/2558 [==============================] - 88s 34ms/step - loss: 1.9108\n",
      "Epoch 9/10\n",
      "2558/2558 [==============================] - ETA: 0s - loss: 1.8601\n",
      "Epoch 00009: loss improved from 1.91081 to 1.86006, saving model to weights-improvement-bigger.hdf5\n",
      "2558/2558 [==============================] - 88s 35ms/step - loss: 1.8601\n",
      "Epoch 10/10\n",
      "2558/2558 [==============================] - ETA: 0s - loss: 1.8172\n",
      "Epoch 00010: loss improved from 1.86006 to 1.81723, saving model to weights-improvement-bigger.hdf5\n",
      "2558/2558 [==============================] - 89s 35ms/step - loss: 1.8172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ad902d12c8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=10, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163781\n",
      "Total Vocab:  59\n",
      "Total Patterns:  163681\n",
      "Seed:\n",
      "\"  tops\n",
      "of the trees under which she had been wandering, when a sharp hiss made\n",
      "her draw back in a hur \"\n",
      "py of the court of the court. \n",
      "'i dane was the mittle miat the sablo of the say ii the sealons ' she karch hare seid the rueen. \n",
      "'i dane was the mittle mittle thing the sealons ' she karch hare seid the rueen. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to the mittle bould ne the sealons ' said the king. \n",
      "'i dan to\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
